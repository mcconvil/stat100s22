---
title: Theory-Based Inference, Day II
output:
  xaringan::moon_reader:
    css: ["more.css", "xaringan-themer.css", "hygge"]
    lib_dir: libsSlides
    self_contained: false
    nature:
      highlightStyle: github
      ratio: '16:9'      
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false
    seal: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,
                      message = FALSE, 
                      fig.retina = 3, fig.align = 'center',
                      fig.asp = 0.75, fig.width = 8,
                      cache = TRUE)
library(knitr)
library(kableExtra)
library(tidyverse)
theme_update(text = element_text(size = 20),
             plot.title = element_text(hjust = 0.5))
```

```{r xaringan-scribble, echo=FALSE}
xaringanExtra::use_scribble()
```



background-image: url("img/DAW.png")
background-position: left
background-size: 50%
class: middle, center, inverse


.pull-right[



## .whitish[Theory-Based Inference]

## .whitish[and]

## .whitish[Sample Size Calculations]


<br>

### .whitish[Kelly McConville]

#### .yellow[ Stat 100 | Week 11 | Spring 2022] 

]


---

background-image: url("img/ggparty.001.jpeg")
background-position: contain
background-size: 90%


---

background-image: url("img/ggparty.002.jpeg")
background-position: contain
background-size: 90%


---

background-image: url("img/ggparty.003.jpeg")
background-position: contain
background-size: 90%


---

class: middle, center

### If you are able to attend, please RSVP here:

### [bit.ly/stat100-ggparty](https://bit.ly/stat100-ggparty)

---

### Announcements

* Project Assignment 3 is due Friday, April 22nd at 5pm

****************************

--

### Goals for Today

.pull-left[


* Theory-based inference

* Sample size calculations

] 



.pull-right[

* Paired data

* Lots of examples


]

---

### Recap:

**Central Limit Theorem (CLT):** For random samples and a large sample size $(n)$, the sampling distribution of many sample statistics is approximately normal.

--

.pull-left[

#### Sample Proportion Version

When $n$ is large (at least 10 successes and 10 failures):

$$
\hat{p} \sim N \left(p, \sqrt{\frac{p(1-p)}{n}} \right)
$$

]

--

.pull-right[

#### Sample Mean Version

When $n$ is large (at least 30):

$$
\bar{x} \sim N \left(\mu, \frac{\sigma}{\sqrt{n}} \right)
$$
]


---

### But There Are [Several Versions](https://mcconvil.github.io/stat100s22/inference_procedures.html)!

```{r, echo = FALSE}
symbols <- data.frame(Response = c("quantitative", "categorical",
                                   "quantitative", "categorical",
                                   "quantitative"),
                         Explanatory = c("-", "-",
                                         "categorical",
                                         "categorical",
                                         "quantitative"),
                         Numerical_Quantity = c("mean", 
                                                "proportion",
                                                "difference in means",
                                                "difference in proportions",
                                                "correlation"),
                         Parameter = c("$\\mu$", "$p$", 
                                       "$\\mu_1 - \\mu_2$",
                                       "$p_1 - p_2$",
                                       "$\\rho$"),
                                       Statistic = c("$\\bar{x}$",
                                                     "$\\hat{p}$",
                                                     "$\\bar{x}_1 - \\bar{x}_2$",
                                                     "$\\hat{p}_1 - \\hat{p}_2$",
                                                     "$r$"))

symbols %>% 
  kable()  %>%
  kable_styling(bootstrap_options = c("responsive", "bordered")) 
```



---

### Recap:

**Z-score test statistics**: 

$$
\mbox{Z-score} = \frac{\mbox{statistic} - \mu}{\sigma}
$$

--

* Usually follows a **standard normal** or a **t** distribution.

--

* Use the approximate distribution to find the p-value.

---

### Recap:

**Formula-Based** P*100% Confidence Intervals

$$
\mbox{statistic} \pm z^* SE
$$

where $P(-z^* \leq Z \leq z^*) = P$

--

OR

$$
\mbox{statistic} \pm t^* SE
$$

where $P(-t^* \leq t \leq t^*) = P$


---

class: inverse, middle, center


## How do we do probability model calculations in `R`?


---

### Probability Calculations in R


**Question**: How do I compute **probabilities** in R?

.pull-left[

```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = 1),
         z = if_else(x > 1, TRUE, FALSE))
dat2 <- dat %>%
  filter(z)
ggplot(data = dat, mapping = aes(x, y)) +
  geom_ribbon(mapping = aes(ymin = 0, ymax = y, fill = z),
              data = dat2) +
  guides(fill = FALSE) +
  geom_line(size = 2) +
  scale_fill_manual(values = "deeppink")
```

]

--

.pull-right[

```{r}
pnorm(q = 1, mean = 0, sd = 1)
pt(q = 1, df = 52)
```

]

**Doesn't seem quite right**...

---

### Probability Calculations in R


**Question**: How do I compute **probabilities** in R?

.pull-left[

```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = 1),
         z = if_else(x > 1, TRUE, FALSE))
dat2 <- dat %>%
  filter(z)
ggplot(data = dat, mapping = aes(x, y)) +
  geom_ribbon(mapping = aes(ymin = 0, ymax = y, fill = z),
              data = dat2) +
  guides(fill = FALSE) +
  geom_line(size = 2) +
  scale_fill_manual(values = "deeppink")
```

]

.pull-right[

```{r}
pnorm(q = 1, mean = 0, sd = 1,
      lower.tail = FALSE)
pt(q = 1, df = 52, lower.tail = FALSE)
```

]


---

### P*100% CI for parameter:

$$
\mbox{statistic} \pm z^* SE
$$

.pull-left[

**Question**: How do I find the correct critical values $(z^* \mbox{ or } t^*)$ for the confidence interval?

```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = 1))
ggplot(data = dat, mapping = aes(x, y)) +
  geom_line(size = 2) +
  geom_vline(xintercept = -1.96, color = "deeppink", size = 2) +
  geom_vline(xintercept = 1.96, color = "deeppink", size = 2)
```

]


--

.pull-right[

```{r}
qnorm(p = 0.975, mean = 0, sd = 1)
qt(p = 0.975, df = 52)
```

]

---

### P*100% CI for parameter:

$$
\mbox{statistic} \pm z^* SE
$$
.pull-left[

**Question**: What percentile/quantile do I need for a 90% CI?

```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = 1))
ggplot(data = dat, mapping = aes(x, y)) +
  geom_line(size = 2) +
  geom_vline(xintercept = -1.645, color = "deeppink", size = 2) +
  geom_vline(xintercept = 1.645, color = "deeppink", size = 2)
```

]

--

.pull-right[

```{r}
qnorm(p = 0.95, mean = 0, sd = 1)
qt(p = 0.95, df = 52)
```

]

---

### Probability Calculations in R

**To help you remember**:

Want a **P**robability? 

--

&rarr; use `pnorm()`, `pt()`, ...

--

Want a **Q**uantile (i.e. percentile)?

--

&rarr; use `qnorm()`, `qt()`, ...


---

### Probability Calculations in R

**Question**: When might I want to do probability calculations in R?

--
    
&rarr; Computed a test statistic that is approximated by a named random variable.  Want to compute the p-value with `p---()`

--

&rarr; Compute a confidence interval.  Want to find the critical value with `q---()`.

--

&rarr; To do a **Sample Size Calculation**.

---

### Sample Size Calculations

* Very important part of the data analysis process!

--

* Happens BEFORE you collect data.

--

* You determine how large your sample size needs for a desired precision in your CI.
    + There is also a hypothesis test version that we won't be covering in Stat 100.
    
---

### Sample Size Calculations

**Question**: Why do we need sample size calculations?

--

**Example**: Let's return to the dolphins for treating depression example.  

--

With a sample size of 30 and 95% confidence, we estimate that the improvement rate for depression is between 14.5 percentage points and 75 percentage points higher if you swim with a dolphin instead of doing yoga.

--

With a width of 60.5 percentage points, this 95% CI is a **wide**/very imprecise interval.  

--

**Question**: How could we make it narrower?  How could we decrease the Margin of Error (ME)?

--

&rarr; **Decrease** the confidence level!

--

&rarr; **Increase** the sample size!


---

### Sample Size Calculations -- Single Proportion

Let's focus on estimating a single proportion.  Suppose we want to estimate the current proportion of Harvard undergraduates with COVID with 95% confidence and we want the margin of error on our interval to be less than or equal to 0.02.  **How large does our sample size need to be?**

--

Want 

$$ 
z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \leq B
$$

--

Need to derive a formula that looks like 

$$ 
n \geq \quad ...
$$

--

**Question**: How can we isolate $n$ to be on a side by itself?


---

### Sample Size Calculations -- Single Proportion

Let's focus on estimating a single proportion.  Suppose we want to estimate the current proportion of Harvard undergraduates with COVID with 95% confidence and we want the margin of error on our interval to be less than or equal to 0.02. **How large does our sample size need to be?**

**Sample size calculation:**

$$
n \geq \frac{\hat{p}(1 - \hat{p})z^{*2}}{B^2}
$$
--


* What do we plug in for, $\hat{p}$, $z^{*}$, $B$?

--

* Will consider sample size calculations when estimating a **mean** on this week's p-set.

---

class: middle, center, inverse

## Another important concept: Paired data


---

### Paired Data: Mean Difference

**Example**: Is the mean number of free throw attempts awarded to the Miami Heat during games different from the mean number attempted by their opponents?

```{r}
library(tidyverse)
library(Lock5Data)
# Data
data("MiamiHeat")
select(MiamiHeat, Game, Location, Opp, FTA, OppFTA) %>%
  slice(1:6)
```



* Variables of interest:


* Parameter of interest:


---

### Paired Data: Mean Difference


```{r}
select(MiamiHeat, Game, Location, Opp, FTA, OppFTA) %>%
  slice(1:6)
```

What are the cases?

--

How could we control for case-to-case variability?

---

### Paired Data: Mean Difference

* Paired data: **repeated** observations on the same case

--

* Paired data should not be treated as **independent** observations
    + Why not?

--
    
* **Benefit of pairing**: By accounting for the case-to-case variability, any differences we see are more directly related to the explanatory variable.

---

### Paired Data: Mean Difference

.pull-left[

```{r heat, fig.show = 'hide'}
# Calculate Difference
MiamiHeat <- MiamiHeat %>%
  mutate(diff_FTA = FTA - OppFTA)

# Visualize
ggplot(data = MiamiHeat, 
       mapping = aes(x = diff_FTA)) +
  geom_histogram()
```

]

.pull-right[

```{r, echo = FALSE}
knitr::include_graphics(knitr::fig_chunk("heat", "png"))
```

]

---

## Conducting a theory-based t-test

.pull-left[

```{r}
# One-sample t-test
t.test(x = MiamiHeat$diff_FTA)
```

]

.pull-right[

```{r}
# Two-sample t-test
# Ignoring the pairing
t.test(x = MiamiHeat$FTA,
       y = MiamiHeat$OppFTA)
```


]

---

### Assumptions

All these methods that rely on the CLT, assume:

* The sample size is large.

--

* The sample is a random sample.
    + Observations are independent of each other.



---

class: inverse, center, middle

### Let's go through more examples!



