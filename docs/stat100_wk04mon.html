<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Collection</title>
    <meta charset="utf-8" />
    <script src="libsSlides/header-attrs-2.11/header-attrs.js"></script>
    <link href="libsSlides/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <script src="libsSlides/fabric-4.3.1/fabric.min.js"></script>
    <link href="libsSlides/xaringanExtra-scribble-0.0.1/scribble.css" rel="stylesheet" />
    <script src="libsSlides/xaringanExtra-scribble-0.0.1/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link rel="stylesheet" href="more.css" type="text/css" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">








background-image: url("img/DAW.png")
background-position: left
background-size: 50%
class: middle, center, inverse


.pull-right[



## .whitish[Data Collection]

&lt;br&gt;

&lt;br&gt;

### .whitish[Kelly McConville]

#### .yellow[ Stat 100 | Week 4 | Spring 2022] 

]



---

## Announcements


* Make sure to start working on Project Assignment 1 with your group members.
* P-Set 3 due now on Friday at 5pm because of server hiccup.



****************************

--

## Goals for Today

.pull-left[

* Go through data joins.

] 

--

.pull-right[


* Discuss data collection/acquisition.

]


---

## Data Joins

* Often in the data analysis workflow, we have more than one data source, which means more than one dataframe, and we want to combine these dataframes.

--

* Need principled way to combine.
    + Need a **key** that links two dataframes together.

--

* These multiple dataframes are called **relational data**.

&lt;!-- --- --&gt;

&lt;!-- ## Data Joins: Language --&gt;

&lt;!-- Three families of verbs for working with relational data: --&gt;

&lt;!-- -- --&gt;

&lt;!-- * **Mutating joins**: Add variables to one data frame but matching observations in another. --&gt;

&lt;!-- -- --&gt;

&lt;!-- * **Filtering joins**: Filter observations from one data frame based on whether or not they match an observation in the other table. --&gt;

&lt;!-- -- --&gt;

&lt;!-- * **Set operations**: treat observations as if they were set elements. --&gt;

    
---

## Example: BLS Consumer Expenditure Data

* Household survey but data are also collected on individuals
    + fmli: household data
    + memi: household member-level data


```r
#Read in data with readr package
library(tidyverse)
fmli &lt;- read_csv("~/shared_data/stat100/data/fmli.csv", 
                 na = c("NA", "."))
memi &lt;- read_csv("~/shared_data/stat100/data/memi.csv", 
                 na = c("NA", "."))
```

* Want variables on principal earner to be added to the household data

---

## CE Data

* Key variable(s)?


```r
library(dplyr)
glimpse(fmli)
```

```
## Rows: 6,301
## Columns: 51
## $ NEWID    &lt;chr&gt; "03324174", "03324204", "03324214", "03324244", "03324274", "…
## $ PRINEARN &lt;chr&gt; "01", "01", "01", "01", "02", "01", "01", "01", "02", "01", "…
## $ FINLWT21 &lt;dbl&gt; 25984.767, 6581.018, 20208.499, 18078.372, 20111.619, 19907.3…
## $ FINCBTAX &lt;dbl&gt; 116920, 200, 117000, 0, 2000, 942, 0, 91000, 95000, 40037, 10…
## $ BLS_URBN &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ POPSIZE  &lt;dbl&gt; 2, 3, 4, 2, 2, 2, 1, 2, 5, 2, 3, 2, 2, 3, 4, 3, 3, 1, 4, 1, 1…
## $ EDUC_REF &lt;chr&gt; "16", "15", "16", "15", "14", "11", "10", "13", "12", "12", "…
## $ EDUCA2   &lt;dbl&gt; 15, 15, 13, NA, NA, NA, NA, 15, 15, 14, 12, 12, NA, NA, NA, 1…
## $ AGE_REF  &lt;dbl&gt; 63, 50, 47, 37, 51, 63, 77, 37, 51, 64, 26, 59, 81, 51, 67, 4…
## $ AGE2     &lt;dbl&gt; 50, 47, 46, NA, NA, NA, NA, 36, 53, 67, 44, 62, NA, NA, NA, 4…
## $ SEX_REF  &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1…
## $ SEX2     &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 1, 1, 1, NA, NA, NA, 1, NA, 1,…
## $ REF_RACE &lt;dbl&gt; 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1…
## $ RACE2    &lt;dbl&gt; 1, 4, 1, NA, NA, NA, NA, 1, 1, 1, 1, 1, NA, NA, NA, 2, NA, 1,…
## $ HISP_REF &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1…
## $ HISP2    &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 2, 2, 2, NA, NA, NA, 2, NA, 2,…
## $ FAM_TYPE &lt;dbl&gt; 3, 4, 1, 8, 9, 9, 8, 3, 1, 1, 3, 1, 8, 9, 8, 5, 9, 4, 8, 3, 2…
## $ MARITAL1 &lt;dbl&gt; 1, 1, 1, 5, 3, 3, 2, 1, 1, 1, 1, 1, 2, 3, 5, 1, 3, 1, 3, 1, 1…
## $ REGION   &lt;dbl&gt; 4, 4, 3, 4, 4, 3, 4, 1, 3, 2, 1, 4, 1, 3, 3, 3, 2, 1, 2, 4, 3…
## $ SMSASTAT &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ HIGH_EDU &lt;chr&gt; "16", "15", "16", "15", "14", "11", "10", "15", "15", "14", "…
## $ EHOUSNGC &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ TOTEXPCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ FOODCQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ TRANSCQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ HEALTHCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ ENTERTCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EDUCACQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ TOBACCCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ STUDFINX &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ IRAX     &lt;dbl&gt; 1000000, 10000, 0, NA, NA, 0, 0, 15000, NA, 477000, NA, NA, N…
## $ CUTENURE &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 4, 1, 1, 2, 1, 2, 2, 2, 2, 4, 1, 1, 1, 4, 4…
## $ FAM_SIZE &lt;dbl&gt; 4, 6, 2, 1, 2, 2, 1, 5, 2, 2, 4, 2, 1, 2, 1, 4, 2, 4, 1, 3, 3…
## $ VEHQ     &lt;dbl&gt; 3, 5, 0, 4, 2, 0, 0, 2, 4, 2, 3, 2, 1, 3, 1, 2, 4, 4, 0, 2, 3…
## $ ROOMSQ   &lt;dbl&gt; 8, 5, 6, 4, 4, 4, 7, 5, 4, 9, 6, 10, 4, 7, 5, 6, 6, 8, 18, 4,…
## $ INC_HRS1 &lt;dbl&gt; 40, 40, 40, 44, 40, NA, NA, 40, 40, NA, 40, NA, NA, NA, NA, 4…
## $ INC_HRS2 &lt;dbl&gt; 30, 40, 52, NA, NA, NA, NA, 40, 40, NA, 65, NA, NA, NA, NA, 6…
## $ EARNCOMP &lt;dbl&gt; 3, 2, 2, 1, 4, 7, 8, 2, 2, 8, 2, 8, 8, 7, 8, 2, 7, 3, 1, 2, 1…
## $ NO_EARNR &lt;dbl&gt; 4, 2, 2, 1, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2, 1, 3, 1, 2, 1…
## $ OCCUCOD1 &lt;chr&gt; "03", "03", "05", "03", "04", "", "", "12", "04", "", "01", "…
## $ OCCUCOD2 &lt;chr&gt; "04", "02", "01", "", "", "", "", "02", "03", "", "11", "", "…
## $ STATE    &lt;chr&gt; "41", "15", "48", "06", "06", "48", "06", "42", "", "27", "25…
## $ DIVISION &lt;dbl&gt; 9, 9, 7, 9, 9, 7, 9, 2, NA, 4, 1, 8, 2, 5, 6, 7, 3, 2, 3, 9, …
## $ TOTXEST  &lt;dbl&gt; 15452, 11459, 15738, 25978, 588, 0, 0, 7261, 9406, -1414, 141…
## $ CREDFINX &lt;dbl&gt; 0, NA, 0, NA, 5, NA, NA, NA, NA, 0, NA, 0, NA, NA, NA, 2, 35,…
## $ CREDITB  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ CREDITX  &lt;dbl&gt; 4000, 5000, 2000, NA, 7000, 1800, NA, 6000, NA, 719, NA, 1200…
## $ BUILDING &lt;chr&gt; "01", "01", "01", "02", "08", "01", "01", "01", "01", "01", "…
## $ ST_HOUS  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
## $ INT_PHON &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ INT_HOME &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
```

---


## CE Data

* Key variables?


```r
glimpse(memi)
```

```
## Rows: 15,412
## Columns: 14
## $ NEWID    &lt;chr&gt; "03552611", "03552641", "03552641", "03552651", "03552651", "…
## $ MEMBNO   &lt;dbl&gt; 1, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3…
## $ AGE      &lt;dbl&gt; 58, 54, 49, 39, 10, 32, 7, 9, 38, 34, 11, 8, 6, 3, 65, 61, 11…
## $ SEX      &lt;dbl&gt; 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1…
## $ EARNER   &lt;dbl&gt; 1, 1, 2, 2, NA, 2, NA, NA, 1, 2, NA, NA, NA, NA, 1, 2, NA, NA…
## $ EARNTYPE &lt;dbl&gt; 2, 1, NA, NA, NA, NA, NA, NA, 3, NA, NA, NA, NA, NA, 4, NA, N…
## $ INC_HRSQ &lt;dbl&gt; 20, 56, NA, NA, NA, NA, NA, NA, 50, NA, NA, NA, NA, NA, 25, N…
## $ INCOMEY  &lt;dbl&gt; 4, 1, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, 4, NA, N…
## $ OCCUCODE &lt;chr&gt; "10", "05", "", "", "", "", "", "", "03", "", "", "", "", "",…
## $ HISPANIC &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ MEMBRACE &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1…
## $ PAYSTUB  &lt;dbl&gt; 1, 2, NA, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, 2, NA, N…
## $ SALARYX  &lt;dbl&gt; 8982, NA, NA, NA, NA, NA, NA, NA, 280500, NA, NA, NA, NA, NA,…
## $ WKSTATUS &lt;dbl&gt; 1, 1, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, 1, NA, N…
```

---

## CE Data

* Key variables?
    + Problem with class?


```r
glimpse(select(fmli, 1,2))
```

```
## Rows: 6,301
## Columns: 2
## $ NEWID    &lt;chr&gt; "03324174", "03324204", "03324214", "03324244", "03324274", "…
## $ PRINEARN &lt;chr&gt; "01", "01", "01", "01", "02", "01", "01", "01", "02", "01", "…
```

```r
glimpse(select(memi, 1:2))
```

```
## Rows: 15,412
## Columns: 2
## $ NEWID  &lt;chr&gt; "03552611", "03552641", "03552641", "03552651", "03552651", "03…
## $ MEMBNO &lt;dbl&gt; 1, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, …
```

---

## CE Data

* Key variables?
    + Problem with class?


```r
fmli &lt;- mutate(fmli, PRINEARN = as.integer(PRINEARN))
glimpse(select(fmli, 1, 2))
```

```
## Rows: 6,301
## Columns: 2
## $ NEWID    &lt;chr&gt; "03324174", "03324204", "03324214", "03324244", "03324274", "…
## $ PRINEARN &lt;int&gt; 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1…
```

```r
glimpse(select(memi, 1, 2))
```

```
## Rows: 15,412
## Columns: 2
## $ NEWID  &lt;chr&gt; "03552611", "03552641", "03552641", "03552651", "03552651", "03…
## $ MEMBNO &lt;dbl&gt; 1, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, …
```

---

## CE Data

* Want to add columns of `memi` to `fmli` that correspond to the principal earner's memi data
    + What type of join is that?

---

## The World of Joins

* Mutating joins: Add new variables to one dataset from matching observations in another.
    + `left_join()` (and `right_join()`)
    + `inner_join()`
    + `full_join()`

* There are also *filtering* joins but we won't cover those today.    

---

## Example Dataframes


```r
d1 &lt;- tibble(V = 1:5, X = c(1, 2, 2, 3, 1), Y = c(14, 3, 6, 1, 4))
d2 &lt;- tibble(X = c(2, 4, 1), S = c(4, 13, 8))
d1
```

```
## # A tibble: 5 × 3
##       V     X     Y
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14
## 2     2     2     3
## 3     3     2     6
## 4     4     3     1
## 5     5     1     4
```

```r
d2
```

```
## # A tibble: 3 × 2
##       X     S
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     2     4
## 2     4    13
## 3     1     8
```

---

## `left_join()`


```r
d1_new &lt;- left_join(d1, d2)
```

```
## Joining, by = "X"
```

```r
d1_new
```

```
## # A tibble: 5 × 4
##       V     X     Y     S
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6     4
## 4     4     3     1    NA
## 5     5     1     4     8
```

---

## `left_join()`


```r
d1_new &lt;- left_join(d1, d2, by = c("X" = "X"))
d1_new
```

```
## # A tibble: 5 × 4
##       V     X     Y     S
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6     4
## 4     4     3     1    NA
## 5     5     1     4     8
```

---

## `left_join()`


```r
d1_new &lt;- left_join(d1, d2, by = c("V" = "X"))
d1_new
```

```
## # A tibble: 5 × 4
##       V     X     Y     S
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6    NA
## 4     4     3     1    13
## 5     5     1     4    NA
```

---

## `inner_join()`


```r
d1_d2 &lt;- inner_join(d1, d2, by = c("X" = "X"))
d1_d2
```

```
## # A tibble: 4 × 4
##       V     X     Y     S
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6     4
## 4     5     1     4     8
```

---

## `inner_join()`


```r
d1_d2 &lt;- inner_join(d1, d2, by = c("V" = "X"))
d1_d2
```

```
## # A tibble: 3 × 4
##       V     X     Y     S
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     4     3     1    13
```

---

## `full_join()`


```r
d1_d2 &lt;- full_join(d1, d2, by = c("X" = "X"))
d1_d2
```

```
## # A tibble: 6 × 4
##       V     X     Y     S
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6     4
## 4     4     3     1    NA
## 5     5     1     4     8
## 6    NA     4    NA    13
```

---

## `full_join()`


```r
d1_d2 &lt;- full_join(d1, d2, by = c("V" = "X"))
d1_d2
```

```
## # A tibble: 5 × 4
##       V     X     Y     S
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     1    14     8
## 2     2     2     3     4
## 3     3     2     6    NA
## 4     4     3     1    13
## 5     5     1     4    NA
```

---

## Back to our Example

* What kind of join do we want for the Consumer Expenditure data?
    + Want to add columns of `memi` to `fmli` that correspond to the principal earner's memi data

---

## Look at the Possible Joins


```r
left_join(fmli, memi) %&gt;% 
  arrange(NEWID)
```

```
## Joining, by = "NEWID"
```

```
## # A tibble: 15,412 × 64
##    NEWID    PRINEARN FINLWT21 FINCBTAX BLS_URBN POPSIZE EDUC_REF EDUCA2 AGE_REF
##    &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 03324174        1   25985.   116920        1       2 16           15      63
##  2 03324174        1   25985.   116920        1       2 16           15      63
##  3 03324174        1   25985.   116920        1       2 16           15      63
##  4 03324174        1   25985.   116920        1       2 16           15      63
##  5 03324204        1    6581.      200        1       3 15           15      50
##  6 03324204        1    6581.      200        1       3 15           15      50
##  7 03324204        1    6581.      200        1       3 15           15      50
##  8 03324204        1    6581.      200        1       3 15           15      50
##  9 03324204        1    6581.      200        1       3 15           15      50
## 10 03324204        1    6581.      200        1       3 15           15      50
## # … with 15,402 more rows, and 55 more variables: AGE2 &lt;dbl&gt;, SEX_REF &lt;dbl&gt;,
## #   SEX2 &lt;dbl&gt;, REF_RACE &lt;dbl&gt;, RACE2 &lt;dbl&gt;, HISP_REF &lt;dbl&gt;, HISP2 &lt;dbl&gt;,
## #   FAM_TYPE &lt;dbl&gt;, MARITAL1 &lt;dbl&gt;, REGION &lt;dbl&gt;, SMSASTAT &lt;dbl&gt;,
## #   HIGH_EDU &lt;chr&gt;, EHOUSNGC &lt;dbl&gt;, TOTEXPCQ &lt;dbl&gt;, FOODCQ &lt;dbl&gt;,
## #   TRANSCQ &lt;dbl&gt;, HEALTHCQ &lt;dbl&gt;, ENTERTCQ &lt;dbl&gt;, EDUCACQ &lt;dbl&gt;,
## #   TOBACCCQ &lt;dbl&gt;, STUDFINX &lt;dbl&gt;, IRAX &lt;dbl&gt;, CUTENURE &lt;dbl&gt;, FAM_SIZE &lt;dbl&gt;,
## #   VEHQ &lt;dbl&gt;, ROOMSQ &lt;dbl&gt;, INC_HRS1 &lt;dbl&gt;, INC_HRS2 &lt;dbl&gt;, EARNCOMP &lt;dbl&gt;, …
```

---

## Look at the Possible Joins

* Be careful.  This erroneous example made my R crash!


```r
left_join(fmli, memi, by = c("PRINEARN" = "MEMBNO")) %&gt;% 
  arrange(MEMBNO)
```

---

## Look at the Possible Joins


```r
left_join(fmli, memi, by = c("NEWID" = "NEWID",
                             "PRINEARN" = "MEMBNO")) %&gt;% 
  arrange(NEWID)
```

```
## # A tibble: 6,301 × 63
##    NEWID    PRINEARN FINLWT21 FINCBTAX BLS_URBN POPSIZE EDUC_REF EDUCA2 AGE_REF
##    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 03324174        1   25985.   116920        1       2 16           15      63
##  2 03324204        1    6581.      200        1       3 15           15      50
##  3 03324214        1   20208.   117000        1       4 16           13      47
##  4 03324244        1   18078.        0        1       2 15           NA      37
##  5 03324274        2   20112.     2000        1       2 14           NA      51
##  6 03324284        1   19907.      942        1       2 11           NA      63
##  7 03324294        1   11705.        0        1       1 10           NA      77
##  8 03324304        1   24431.    91000        1       2 13           15      37
##  9 03324324        2   42859.    95000        2       5 12           15      51
## 10 03324334        1   17481.    40037        1       2 12           14      64
## # … with 6,291 more rows, and 54 more variables: AGE2 &lt;dbl&gt;, SEX_REF &lt;dbl&gt;,
## #   SEX2 &lt;dbl&gt;, REF_RACE &lt;dbl&gt;, RACE2 &lt;dbl&gt;, HISP_REF &lt;dbl&gt;, HISP2 &lt;dbl&gt;,
## #   FAM_TYPE &lt;dbl&gt;, MARITAL1 &lt;dbl&gt;, REGION &lt;dbl&gt;, SMSASTAT &lt;dbl&gt;,
## #   HIGH_EDU &lt;chr&gt;, EHOUSNGC &lt;dbl&gt;, TOTEXPCQ &lt;dbl&gt;, FOODCQ &lt;dbl&gt;,
## #   TRANSCQ &lt;dbl&gt;, HEALTHCQ &lt;dbl&gt;, ENTERTCQ &lt;dbl&gt;, EDUCACQ &lt;dbl&gt;,
## #   TOBACCCQ &lt;dbl&gt;, STUDFINX &lt;dbl&gt;, IRAX &lt;dbl&gt;, CUTENURE &lt;dbl&gt;, FAM_SIZE &lt;dbl&gt;,
## #   VEHQ &lt;dbl&gt;, ROOMSQ &lt;dbl&gt;, INC_HRS1 &lt;dbl&gt;, INC_HRS2 &lt;dbl&gt;, EARNCOMP &lt;dbl&gt;, …
```

---

## Look at the Possible Joins


```r
inner_join(fmli, memi, by = c("NEWID" = "NEWID",
                              "PRINEARN" = "MEMBNO")) %&gt;% 
  arrange(NEWID)
```

```
## # A tibble: 6,301 × 63
##    NEWID    PRINEARN FINLWT21 FINCBTAX BLS_URBN POPSIZE EDUC_REF EDUCA2 AGE_REF
##    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 03324174        1   25985.   116920        1       2 16           15      63
##  2 03324204        1    6581.      200        1       3 15           15      50
##  3 03324214        1   20208.   117000        1       4 16           13      47
##  4 03324244        1   18078.        0        1       2 15           NA      37
##  5 03324274        2   20112.     2000        1       2 14           NA      51
##  6 03324284        1   19907.      942        1       2 11           NA      63
##  7 03324294        1   11705.        0        1       1 10           NA      77
##  8 03324304        1   24431.    91000        1       2 13           15      37
##  9 03324324        2   42859.    95000        2       5 12           15      51
## 10 03324334        1   17481.    40037        1       2 12           14      64
## # … with 6,291 more rows, and 54 more variables: AGE2 &lt;dbl&gt;, SEX_REF &lt;dbl&gt;,
## #   SEX2 &lt;dbl&gt;, REF_RACE &lt;dbl&gt;, RACE2 &lt;dbl&gt;, HISP_REF &lt;dbl&gt;, HISP2 &lt;dbl&gt;,
## #   FAM_TYPE &lt;dbl&gt;, MARITAL1 &lt;dbl&gt;, REGION &lt;dbl&gt;, SMSASTAT &lt;dbl&gt;,
## #   HIGH_EDU &lt;chr&gt;, EHOUSNGC &lt;dbl&gt;, TOTEXPCQ &lt;dbl&gt;, FOODCQ &lt;dbl&gt;,
## #   TRANSCQ &lt;dbl&gt;, HEALTHCQ &lt;dbl&gt;, ENTERTCQ &lt;dbl&gt;, EDUCACQ &lt;dbl&gt;,
## #   TOBACCCQ &lt;dbl&gt;, STUDFINX &lt;dbl&gt;, IRAX &lt;dbl&gt;, CUTENURE &lt;dbl&gt;, FAM_SIZE &lt;dbl&gt;,
## #   VEHQ &lt;dbl&gt;, ROOMSQ &lt;dbl&gt;, INC_HRS1 &lt;dbl&gt;, INC_HRS2 &lt;dbl&gt;, EARNCOMP &lt;dbl&gt;, …
```

* Why does this give us the same answer as `left_join` for this situation?

---

## Look at the Possible Joins


```r
full_join(fmli, memi, by = c("NEWID" = "NEWID",
                             "PRINEARN" = "MEMBNO")) %&gt;% 
  arrange(NEWID)
```

```
## # A tibble: 15,412 × 63
##    NEWID    PRINEARN FINLWT21 FINCBTAX BLS_URBN POPSIZE EDUC_REF EDUCA2 AGE_REF
##    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 03324174        1   25985.   116920        1       2 16           15      63
##  2 03324174        2      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  3 03324174        3      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  4 03324174        4      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  5 03324204        1    6581.      200        1       3 15           15      50
##  6 03324204        2      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  7 03324204        3      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  8 03324204        4      NA        NA       NA      NA &lt;NA&gt;         NA      NA
##  9 03324204        5      NA        NA       NA      NA &lt;NA&gt;         NA      NA
## 10 03324204        6      NA        NA       NA      NA &lt;NA&gt;         NA      NA
## # … with 15,402 more rows, and 54 more variables: AGE2 &lt;dbl&gt;, SEX_REF &lt;dbl&gt;,
## #   SEX2 &lt;dbl&gt;, REF_RACE &lt;dbl&gt;, RACE2 &lt;dbl&gt;, HISP_REF &lt;dbl&gt;, HISP2 &lt;dbl&gt;,
## #   FAM_TYPE &lt;dbl&gt;, MARITAL1 &lt;dbl&gt;, REGION &lt;dbl&gt;, SMSASTAT &lt;dbl&gt;,
## #   HIGH_EDU &lt;chr&gt;, EHOUSNGC &lt;dbl&gt;, TOTEXPCQ &lt;dbl&gt;, FOODCQ &lt;dbl&gt;,
## #   TRANSCQ &lt;dbl&gt;, HEALTHCQ &lt;dbl&gt;, ENTERTCQ &lt;dbl&gt;, EDUCACQ &lt;dbl&gt;,
## #   TOBACCCQ &lt;dbl&gt;, STUDFINX &lt;dbl&gt;, IRAX &lt;dbl&gt;, CUTENURE &lt;dbl&gt;, FAM_SIZE &lt;dbl&gt;,
## #   VEHQ &lt;dbl&gt;, ROOMSQ &lt;dbl&gt;, INC_HRS1 &lt;dbl&gt;, INC_HRS2 &lt;dbl&gt;, EARNCOMP &lt;dbl&gt;, …
```

---

## Joining Tips


```r
fmli &lt;- left_join(fmli, memi, by = c("NEWID" = "NEWID",
                                     "PRINEARN" = "MEMBNO")) %&gt;% 
  arrange(NEWID)
```

* FIRST: conceptualize for yourself what you think you want the final dataset to look like!
* Check initial dimensions and final dimensions.
* Use variable names when joining even if they are the same.  


---

class: middle, center

## Quick Survey Time

### Once you have received your number, provide your responses here:

# [https://bit.ly/stat100-survey](https://bit.ly/stat100-survey)

---

class: inverse, middle, center


## Data Collection


&lt;img src="img/twitter-study-design.png" width="50%" style="display: block; margin: auto;" /&gt;



---

##  Who are the data supposed to represent?

&lt;img src="img/week4.002.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

--

**Key questions:**

+ What evidence is there that the data are **representative**?
+ Who is present?  Who is absent?
+ Who is overrepresented?  Who is underrepresented?

---

##  Who are the data supposed to represent?

&lt;img src="img/week4.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

--

**Census**: We have data on the whole population!

---

##  Who are the data supposed to represent?

&lt;img src="img/sampling.002.jpeg" width="90%" style="display: block; margin: auto;" /&gt;

---

##  Who are the data supposed to represent?

&lt;img src="img/week4.005.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

--


**Key questions:**

+ What evidence is there that the data are **representative**?
+ Who is present?  Who is absent?
+ Who is overrepresented?  Who is underrepresented?

---

## Sampling Bias

&lt;img src="img/sampling.001.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

**Sampling bias**: When the sampled units are **systematically different** from the non-sampled units on the variables of interest.

---

### Sampling Bias Example

The **Literary Digest** was a political magazine that correctly predicted the presidential outcomes from 1916 to 1932.  In 1936, they conducted the most extensive (to that date) public opinion poll.  They mailed questionnaires to over **10 million people** (about 1/3 of US households) whose names and addresses they obtained from telephone books and vehicle registration lists.  More than 2.4 million responded with 57% indicating that they would vote for Republican Alf Landon in the upcoming presidential election instead of the current President Franklin Delano Roosevelt.

--

**Population of Interest**:

&lt;br&gt;

**Sample**:

&lt;br&gt;

**Key questions:**

+ What evidence is there that the data are **representative**?
+ Who is present?  Who is absent?
+ Who is overrepresented?  Who is underrepresented?


**Sampling bias**:


---

## Random Sampling

Use random sampling (a random mechanism for selecting cases from the population) to remove sampling bias.


#### Types of random sampling

* Simple random sampling

* Stratified random sampling

* Cluster sampling

--

Why aren't all samples generated using simple random sampling?

---

## National Health and Nutrition Examination Survey (NHANES)

Why are these data collected?

--

&amp;rarr; To assess the health of people in the US.

--

How are these data collected?

--

&amp;rarr; **Stage 1**: US is stratified by geography and distribution of minority populations.  Counties are randomly selected within each stratum.

--

&amp;rarr; **Stage 2**: From the sampled counties, city blocks are randomly selected. (City blocks are clusters.)

--

&amp;rarr; **Stage 3**: From sampled city blocks, households are randomly selected. (Household are clusters.)

--

&amp;rarr; **Stage 4**: From sampled households, people are randomly selected.  For the sampled households, a mobile health vehicle goes to the house and medical professionals take the necessary measurements.

--

**Why don't they use simple random sampling?**

---

### Careful Using Non-SRS Data

.pull-left[

&lt;img src="stat100_wk04mon_files/figure-html/unnamed-chunk-26-1.png" width="576" style="display: block; margin: auto;" /&gt;


]


--

.pull-right[

&lt;img src="stat100_wk04mon_files/figure-html/unnamed-chunk-27-1.png" width="576" style="display: block; margin: auto;" /&gt;

]

--

* If you are dealing with data collected using a complex sampling design, I'd recommend taking an additional stats course, like Stat 160: Sample Surveys!

---

class: middle, center, inverse

## Detour: Data Ethics

---

### Data Ethics

&gt; "Good statistical practice is fundamentally based on transparent assumptions, reproducible results, and valid interpretations." -- Committee on Professional Ethics of the American Statistical Association (ASA)

--

The ASA have created ["Ethical Guidelines for Statistical Practice"](https://www.amstat.org/ASA/Your-Career/Ethical-Guidelines-for-Statistical-Practice.aspx)

--

&amp;rarr; These guidelines are for EVERYONE doing statistical work. 

--

&amp;rarr; There are ethical decisions at all steps of the Data Analysis Process.

--

&amp;rarr; We will periodically refer to specific guidelines throughout this class.

--

&gt; "Above all, professionalism in statistical practice presumes the goal of advancing knowledge while avoiding harm; using statistics in pursuit of unethical ends is inherently unethical."

---

class: inverse, center, middle

##  Responsibilities to Research Subjects


&gt; "The ethical statistician protects and respects the rights and interests of human and animal subjects at all stages of their involvement in a project. This includes respondents to the census or to surveys, those whose data are contained in administrative records, and subjects of physically or psychologically invasive research."

---

##  Responsibilities to Research Subjects


&gt; "Protects the privacy and confidentiality of research subjects and data concerning them, whether obtained from the subjects directly, other persons, or existing records."


&lt;img src="stat100_wk04mon_files/figure-html/unnamed-chunk-28-1.png" width="576" style="display: block; margin: auto;" /&gt;

---

## Detour from Our Detour

--

.pull-left[


```r
library(tidyverse)
library(NHANES)

ggplot(data = NHANES, 
       mapping = aes(x = Age,
                     y = Height)) +
  geom_point(alpha = 0.1) +
  stat_smooth(color = "blue")
```

]

.pull-right[

&lt;img src="stat100_wk04mon_files/figure-html/points-1.png" width="768" style="display: block; margin: auto;" /&gt;

]




---

## Detour from Our Detour


.pull-left[


```r
library(tidyverse)
library(NHANES)
library(emojifont)

NHANES &lt;- mutate(NHANES, 
          heart = fontawesome("fa-heart"))

ggplot(data = NHANES, 
       mapping = aes(x = Age,
                     y = Height,
                     label = heart)) +
  geom_text(alpha = 0.1, color = "red",
            family='fontawesome-webfont',
            size = 8) +
  stat_smooth(color = "lavender")
```

]

.pull-right[

&lt;img src="stat100_wk04mon_files/figure-html/hearts-1.png" width="768" style="display: block; margin: auto;" /&gt;

]


---

class: middle, center, inverse

## Back to Data Collection

---

###  Who are the data supposed to represent?

&lt;img src="img/sampling.002.jpeg" width="90%" style="display: block; margin: auto;" /&gt;


---

###  Who are the data supposed to represent?

&lt;img src="img/week4.006.jpeg" width="80%" style="display: block; margin: auto;" /&gt;


**Key questions:**

+ What evidence is there that the data are **representative**?
+ Who is present?  Who is absent?
+ Who is overrepresented?  Who is underrepresented?

---

## Nonresponse bias


&lt;img src="img/sampling.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

**Nonresponse bias**: The respondents are **systematically** different from the non-respondents for the variables of interest.


---

### Come Back to Literary Digest Example

The **Literary Digest** was a political magazine that correctly predicted the presidential outcomes from 1916 to 1932.  In 1936, they conducted the most extensive (to that date) public opinion poll.  They mailed questionnaires to over **10 million people** (about 1/3 of US households) whose names and addresses they obtained from telephone books and vehicle registration lists.  More than 2.4 million responded with 57% indicating that they would vote for Republican Alf Landon in the upcoming presidential election instead of the current President Franklin Delano Roosevelt.

&lt;br&gt;

**Non-response bias**:


---

## Tackling Nonresponse bias


&lt;img src="img/sampling.003.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

--

&amp;rarr; Use multiple modes and multiple attempts for reaching sampled cases.

--

&amp;rarr; Explore key demographic variables to see how respondents and non-respondents vary.

---

## Is Bigger Always Better?

--

&lt;img src="img/sampling.004.jpeg" width="80%" style="display: block; margin: auto;" /&gt;

--

For our **Literary Digest Example**, Gallup predicted Roosevelt would win based on a survey of **50,000** people, (instead of 2.4 million).

---

### Big Data Paradox


&lt;img src="img/meng.jpg" width="10%" style="float:left; padding:10px" style="display: block; margin: auto;" /&gt;

&gt; "Without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves." -- Xiao-Li Meng


--

**Example:** 

* During Spring of 2021, Delphi-Facebook estimated  vaccine uptake at 70% and U.S. Census estimated it at 67%.

--

* The CDC reported it to be 53%.

--

And, once we learn about **quantifying uncertainty**, we will see that large sample sizes produce very small measures of uncertainty.

--

&gt; "If you have the resources, invest in data quality far more than you invest in data quantity. Bad-quality data is essentially wiping out the power you think you have. That’s always been a problem, but it’s magnified now because we have big data. " -- Xiao-Li Meng

---

## Other Key Random Mechanism: Random Assignment

--

**Random assignment**: Cases are randomly assigned to categories of the **explanatory variable**

* **Response variable**: Variable I want to better understand

* **Explanatory variables**: Variables I think might explain the response variable

--

&amp;rarr; If the data were collected using **random assignment**, then I can determine if the explanatory variable **causes** changes in the response variable.


---

## Causal Inference

Often want to conclude that an explanatory variable causes changes in a response variable but you did not randomly assign the explanatory variable.

--

**Confounding variable**: When the explanatory variable and response variable vary, so does the confounder.

&amp;rarr; Unclear if the explanatory variable or the confounder (or some other variable) is causing changes in the response.


&lt;img src="img/confound.png" width="70%" style="display: block; margin: auto;" /&gt;

---

## Causal Inference

Often want to conclude that an explanatory variable causes changes in a response variable but you did not randomly assign the explanatory variable.



**Confounding variable**: When the explanatory variable and response variable vary, so does the confounder.

&amp;rarr; Unclear if the explanatory variable or the confounder (or some other variable) is causing changes in the response.


&lt;img src="img/confound2.png" width="70%" style="display: block; margin: auto;" /&gt;

---

## Causal Inference

* **Spurious relationship**: Two variables are associated but not causally related
    + In the age of big data, lots of good examples [out there](https://tylervigen.com/spurious-correlations).

--

&gt; "Correlation does not imply causation."

--

&gt;  "Correlation does not imply not causation."

--

* **Causal inference**: Methods for finding causal relationships even when the data were collected without random assignment.


---

## Types of Studies

**Observational Study:** Collect data in a way that doesn't interfere

--

&amp;rarr; **Example**: Studies of hand washing frequency

--

**Experiment:** Interested in causal relationships so utilize random assignment.  Other key features include:

+ Blinding
+ Control group
+ Placebo

--

&amp;rarr; **Example**: COVID vaccine trials


---

## Thoughts on Data Collection

**Random Sampling**


Random sampling is important to ensure the sample is representative of the population.

--

Representativeness isn't about size.

+ Small random samples will tend to be more representative than large non-random samples.

--

How do we draw conclusions about the population from non-random samples?

--

&amp;rarr; Investigate how your sampled cases (and respondents) are systematically different from the non-sampled cases (and non-respondents).

---

## Thoughts on Data Collection

**Random Assignment**

Random assignment allows you to explore **causal** relationships between your explanatory variables and the predictor variables.

--

How do we draw causal conclusions from studies without random assignment?

--

&amp;rarr; With extreme care!  Try to control for all possible confounding variables.

--

&amp;rarr; Discuss the associations/correlations you found.  Use domain knowledge to address potentially causal links.

--

&amp;rarr; Take more stats to learn more about causal inference.


--

**Bottom Line:** We often have to use imperfect data to make decisions.


---

## Two Key Random Mechanisms


&lt;img src="img/ims_ch2.png" width="80%" style="display: block; margin: auto;" /&gt;



---

### Reminders


* **Participation/Engagement:**
    + In class and section
    + Office hours: Must attend **at least one** office hours during the first five weeks of the semester
    + On Slack: **At least two** posts before Spring Break

* Will practice applying data collection ideas in Section this week!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"ratio": "16:9",
"highlightLines": true,
"countIncrementalSlides": false,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
